{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracció keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcions per treballar amb keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#Funcio per treure els emojis d'un text.\n",
    "def deEmojify(text):\n",
    "\n",
    "    string = \"\"\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        for char in word:\n",
    "            if not char in emoji.UNICODE_EMOJI:\n",
    "                string = string + char\n",
    "\n",
    "    return string\n",
    "\n",
    "def removeStopwords(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('@', '', texto)\n",
    "    texto = re.sub(':', '', texto)\n",
    "    texto = re.sub('/', '', texto)\n",
    "    texto = re.sub('!', '', texto)\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('\\.', '', texto)\n",
    "    texto = deEmojify(texto)\n",
    "    blob = TextBlob(texto).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('spanish')]\n",
    "    return(outputlist)\n",
    "\n",
    "def normalizeString(l):\n",
    "    l = l.fillna('').values.tolist()\n",
    "    for user in l:\n",
    "        user[0] = removeStopwords(user[0])\n",
    "        user[1] = removeStopwords(user[1]) \n",
    "    return l\n",
    "\n",
    "def wordDict(l):\n",
    "    dt = {}\n",
    "    for sublist in l:\n",
    "        for item in sublist:\n",
    "            for item2 in item:\n",
    "                if item2 not in dt:\n",
    "                    dt[item2] = 1\n",
    "                else:\n",
    "                    dt[item2] += 1\n",
    "    \n",
    "    for key in list(dt.keys()):\n",
    "        if len(key) <= 1:  \n",
    "            del dt[key] \n",
    "    \n",
    "    return dt\n",
    "\n",
    "def countWords(lt):\n",
    "    mainDict = {}\n",
    "    for emoji in lt:\n",
    "        if emoji in mainDict:\n",
    "            mainDict[emoji] += 1\n",
    "        else:\n",
    "            mainDict[emoji] = 1\n",
    "    return mainDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracció keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "pp = pd.read_csv(\"pp.csv\")\n",
    "psoe = pd.read_csv(\"psoe.csv\")\n",
    "cs = pd.read_csv(\"cs.csv\")\n",
    "vox = pd.read_csv(\"vox.csv\")\n",
    "up = pd.read_csv(\"up.csv\")\n",
    "\n",
    "#agafem de cada dataframe de cada partit els noms dels usuaris i les descripccions \n",
    "voxUD = vox[[\"name\",\"description\"]]\n",
    "ppUD = pp[[\"name\",\"description\"]]\n",
    "csUD = cs[[\"name\",\"description\"]]\n",
    "psoeUD = psoe[[\"name\",\"description\"]]\n",
    "upUD = up[[\"name\",\"description\"]]\n",
    "\n",
    "#per tot el dataset, treiem les stopwords i natejem tots els caracters que no son utils\n",
    "voxWords = normalizeString(voxUD)\n",
    "#fem un diccionari amb totes les paraules recollides\n",
    "voxWordsFlat =  wordDict(voxWords)        \n",
    "#ordenem el diccionari per ocurrencies i eliminem les que tinguin menys de 3\n",
    "voxWordsFlat = {k: v for k, v in sorted(voxWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "voxWordsFlat = {k: v for k, v in voxWordsFlat.items() if v > 2}\n",
    "#fem un llistat de tuples amb la paraula i la quantitat de cops que surt\n",
    "voxWordsFlat = list(voxWordsFlat.items())\n",
    "\n",
    "#repetim el proces per els altres partits\n",
    "\n",
    "ppWords = normalizeString(ppUD)\n",
    "ppWordsFlat =  wordDict(ppWords)        \n",
    "ppWordsFlat = {k: v for k, v in sorted(ppWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "ppWordsFlat = {k: v for k, v in ppWordsFlat.items() if v > 2}\n",
    "ppWordsFlat = list(ppWordsFlat.items())\n",
    "\n",
    "csWords = normalizeString(csUD)\n",
    "csWordsFlat =  wordDict(csWords)        \n",
    "csWordsFlat = {k: v for k, v in sorted(csWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "csWordsFlat = {k: v for k, v in csWordsFlat.items() if v > 2}\n",
    "csWordsFlat = list(csWordsFlat.items())\n",
    "\n",
    "psoeWords = normalizeString(psoeUD)\n",
    "psoeWordsFlat =  wordDict(psoeWords)        \n",
    "psoeWordsFlat = {k: v for k, v in sorted(psoeWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "psoeWordsFlat = {k: v for k, v in psoeWordsFlat.items() if v > 2}\n",
    "psoeWordsFlat = list(psoeWordsFlat.items())\n",
    "\n",
    "upWords = normalizeString(upUD)\n",
    "upWordsFlat =  wordDict(upWords)        \n",
    "upWordsFlat = {k: v for k, v in sorted(upWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "upWordsFlat = {k: v for k, v in upWordsFlat.items() if v > 2}\n",
    "upWordsFlat = list(upWordsFlat.items())\n",
    "\n",
    "\n",
    "#llista amb les keywords y ocurrencies per a cada partit. \n",
    "fList = [voxWordsFlat,ppWordsFlat,csWordsFlat,psoeWordsFlat,upWordsFlat]\n",
    "\n",
    "flatTrans = []\n",
    "for sublist in fList:\n",
    "    for item in sublist:\n",
    "        flatTrans.append(item[0])\n",
    "\n",
    "#Conjunt de les keywords diferents que apareixen (tenint en compte tots els partits politics). Util per quan calculem el tfidf.\n",
    "diffWords = set(flatTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proves per a calcular l'score d'un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DannyTw = \"soy de vox\"\n",
    "\n",
    "DannyTwClean = removeStopwords(DannyTw)\n",
    "DannyDict = countWords(DannyTwClean)\n",
    "DannyList = list(DannyDict.items())\n",
    "\n",
    "\n",
    "\n",
    "wordIdf = {}\n",
    "for word in diffWords:\n",
    "    wordIdf[word] = 0\n",
    "    for index, party in enumerate(fList):\n",
    "        for partyWord, num in party:\n",
    "            if partyWord == word:\n",
    "                wordIdf[word] += 1\n",
    "\n",
    "\n",
    "voxWordNum = np.sum([value for i, value in fList[0]])\n",
    "ppWordNum = np.sum([value for i, value in fList[1]])\n",
    "csWordNum = np.sum([value for i, value in fList[2]])\n",
    "psoeWordNum = np.sum([value for i, value in fList[3]])\n",
    "upWordNum = np.sum([value for i, value in fList[4]])\n",
    "WordPartiesSum = [voxWordNum, ppWordNum, csWordNum, psoeWordNum, upWordNum]\n",
    "\n",
    "scores = {}\n",
    "for index, party in enumerate(fList):\n",
    "    scores[index] = 0\n",
    "    \n",
    "print(DannyList)\n",
    "\n",
    "for i, value in DannyList:\n",
    "    for index, party in enumerate(fList):\n",
    "        for word, num in party:\n",
    "            if i == word:\n",
    "                scores[index] += value * np.log(len(fList)/wordIdf[word]) * num/WordPartiesSum[index]\n",
    "\n",
    "print(scores)\n",
    "list(scores.values())/np.sum(list(scores.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
