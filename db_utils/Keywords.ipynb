{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TextBlob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f9d1e7b62ef4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvoxUD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoxUsers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"user_name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"user_description\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TextBlob'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import TextBlob\n",
    "\n",
    "voxUD = voxUsers[[\"user_name\",\"user_description\"]]\n",
    "ppUD = ppUsers[[\"user_name\",\"user_description\"]]\n",
    "csUD = csUsers[[\"user_name\",\"user_description\"]]\n",
    "psoeUD = psoeUsers[[\"user_name\",\"user_description\"]]\n",
    "upUD = upUsers[[\"user_name\",\"user_description\"]]\n",
    "\n",
    "def removeStopwords(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('@', '', texto)\n",
    "    texto = re.sub(':', '', texto)\n",
    "    texto = re.sub('/', '', texto)\n",
    "    texto = re.sub('!', '', texto)\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('\\.', '', texto)\n",
    "    texto = deEmojify(texto)\n",
    "    blob = TextBlob(texto).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('spanish')]\n",
    "    return(outputlist)\n",
    "\n",
    "def normalizeString(l):\n",
    "    l = l.fillna('').values.tolist()\n",
    "    for user in l:\n",
    "        user[0] = removeStopwords(user[0])\n",
    "        user[1] = removeStopwords(user[1]) \n",
    "    return l\n",
    "\n",
    "def wordDict(l):\n",
    "    dt = {}\n",
    "    for sublist in l:\n",
    "        for item in sublist:\n",
    "            for item2 in item:\n",
    "                if item2 not in dt:\n",
    "                    dt[item2] = 1\n",
    "                else:\n",
    "                    dt[item2] += 1\n",
    "    \n",
    "    for key in list(dt.keys()):\n",
    "        if len(key) <= 1:  \n",
    "            del dt[key] \n",
    "    \n",
    "    return dt\n",
    "\n",
    "voxWords = normalizeString(voxUD)\n",
    "voxWordsFlat =  wordDict(voxWords)        \n",
    "voxWordsFlat = {k: v for k, v in sorted(voxWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "voxWordsFlat = {k: v for k, v in voxWordsFlat.items() if v > 2}\n",
    "voxWordsFlat = list(voxWordsFlat.items())\n",
    "\n",
    "ppWords = normalizeString(ppUD)\n",
    "ppWordsFlat =  wordDict(ppWords)        \n",
    "ppWordsFlat = {k: v for k, v in sorted(ppWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "ppWordsFlat = {k: v for k, v in ppWordsFlat.items() if v > 2}\n",
    "ppWordsFlat = list(ppWordsFlat.items())\n",
    "\n",
    "csWords = normalizeString(csUD)\n",
    "csWordsFlat =  wordDict(csWords)        \n",
    "csWordsFlat = {k: v for k, v in sorted(csWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "csWordsFlat = {k: v for k, v in csWordsFlat.items() if v > 2}\n",
    "csWordsFlat = list(csWordsFlat.items())\n",
    "\n",
    "psoeWords = normalizeString(psoeUD)\n",
    "psoeWordsFlat =  wordDict(psoeWords)        \n",
    "psoeWordsFlat = {k: v for k, v in sorted(psoeWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "psoeWordsFlat = {k: v for k, v in psoeWordsFlat.items() if v > 2}\n",
    "psoeWordsFlat = list(psoeWordsFlat.items())\n",
    "\n",
    "upWords = normalizeString(upUD)\n",
    "upWordsFlat =  wordDict(upWords)        \n",
    "upWordsFlat = {k: v for k, v in sorted(upWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "upWordsFlat = {k: v for k, v in upWordsFlat.items() if v > 2}\n",
    "upWordsFlat = list(upWordsFlat.items())\n",
    "#print(ppWordsFlat.keys(️️️)️️️)\n",
    "#for item in list(ppWordsFlat.keys()):\n",
    "    #print(len(item))\n",
    "    \n",
    "    \n",
    "fList = [voxWordsFlat,ppWordsFlat,csWordsFlat,psoeWordsFlat,upWordsFlat]\n",
    "\n",
    "flatTrans = []\n",
    "for sublist in fList:\n",
    "    for item in sublist:\n",
    "        flatTrans.append(item[0])\n",
    "\n",
    "diffWords = set(flatTrans)\n",
    "\n",
    "\n",
    "parties = [\"VOX\", \"PP\", \"CS\", \"PSOE\", \"UP\"]\n",
    "\n",
    "DannyTw = \"soy de podemos!!!\"\n",
    "\n",
    "wordIdf = {}\n",
    "for word in diffWords:\n",
    "    wordIdf[word] = 0\n",
    "    for index, party in enumerate(fList):\n",
    "        for partyWord, num in party:\n",
    "            if partyWord == word:\n",
    "                wordIdf[word] += 1\n",
    "\n",
    "\n",
    "voxWordNum = np.sum([value for i, value in fList[0]])\n",
    "ppWordNum = np.sum([value for i, value in fList[1]])\n",
    "csWordNum = np.sum([value for i, value in fList[2]])\n",
    "psoeWordNum = np.sum([value for i, value in fList[3]])\n",
    "upWordNum = np.sum([value for i, value in fList[4]])\n",
    "WordPartiesSum = [voxWordNum, ppWordNum, csWordNum, psoeWordNum, upWordNum]\n",
    "\n",
    "scores = {}\n",
    "for index, party in enumerate(fList):\n",
    "    scores[index] = 0\n",
    "    \n",
    "for i, value in DannyList:\n",
    "    for index, party in enumerate(fList):\n",
    "        for word, num in party:\n",
    "            if i == word:\n",
    "                scores[index] += value * np.log(len(fList)/wordIdf[word]) * num/WordPartiesSum[index]\n",
    "\n",
    "print(scores)\n",
    "list(scores.values())/np.sum(list(scores.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[has, sacado, chapata]\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "import emoji\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "#Create spanish tokenizer.\n",
    "nlp = Spanish()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "name = \"De donde has sacado la chapata!!!!!!\"\n",
    "\n",
    "#Function to remove emojis of a text.\n",
    "def remove_emojis(text):\n",
    "    string = \"\"\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        for char in word:\n",
    "            if not char in emoji.UNICODE_EMOJI:\n",
    "                string = string + char\n",
    "    return string\n",
    "\n",
    "#Remove stop words of a text.\n",
    "def removeStopwords(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('@', '', texto)\n",
    "    texto = re.sub(':', '', texto)\n",
    "    texto = re.sub('/', '', texto)\n",
    "    texto = re.sub('!', '', texto)\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('\\.', '', texto)\n",
    "    return texto\n",
    "\n",
    "#Get important words of a text.\n",
    "def getImportantWords(text):\n",
    "    text_without_emojis = remove_emojis(text)\n",
    "    text_cleaned = removeStopwords(text_without_emojis)\n",
    "    tokenized_text = tokenizer(text_cleaned)\n",
    "    text_important_words = [word for word in tokenized_text if word.is_stop == False]\n",
    "    return text_important_words\n",
    "\n",
    "#Return a dictionary with counter of each word in wor counted.\n",
    "def dictWordCount(wordList, wDict):\n",
    "    for word in wordList:\n",
    "        if word in wDict:\n",
    "            wDict[word] += 1\n",
    "        else:\n",
    "            wDict[word] = 1\n",
    "    return wDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
