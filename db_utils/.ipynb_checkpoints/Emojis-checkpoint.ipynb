{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥ emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iairm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import math\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "import json\n",
    "import regex\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcions per a treballar amb emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "#Funcio per treure els emojis d'un text.\n",
    "def deEmojify(text):\n",
    "\n",
    "    string = \"\"\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        for char in word:\n",
    "            if not char in emoji.UNICODE_EMOJI:\n",
    "                string = string + char\n",
    "\n",
    "    return string\n",
    "\n",
    "#Funcio per obtenir un text sense emojis.\n",
    "def Emojify(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list\n",
    "\n",
    "#Funcion que retorna un diccionari amb el count de cada emoji.\n",
    "def countEmojis(text,mainDict):\n",
    "    lt = list(Emojify(text)) # Set deleted\n",
    "    if \"üá™üá¶\" in lt:\n",
    "        lt.remove(\"üá™üá¶\")\n",
    "        lt.append(\"üá™üá∏\")\n",
    "    if \"‚ù§\" in lt:\n",
    "        lt.remove(\"‚ù§\")\n",
    "        lt.append(\"‚ù§Ô∏è\")\n",
    "    for emoji in lt:\n",
    "        if emoji in mainDict:\n",
    "            mainDict[emoji] += 1\n",
    "        else:\n",
    "            mainDict[emoji] = 1\n",
    "    return mainDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis count per a cada partit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "# Llegim els csv de cada partit politic.\n",
    "pp = pd.read_csv(\"pp.csv\")\n",
    "psoe = pd.read_csv(\"psoe.csv\")\n",
    "cs = pd.read_csv(\"cs.csv\")\n",
    "vox = pd.read_csv(\"vox.csv\")\n",
    "up = pd.read_csv(\"up.csv\")\n",
    "\n",
    "finaldu = [] # Llista amb les ocurrencies d'emojis per a cada partit.\n",
    "\n",
    "userDict = {} # Diccionari on guardarem les ocurrencies de cada emoji de tots els usuaris del partit actual.\n",
    "#Vox Users\n",
    "voxUsers = vox[[\"user_name\",\"user_screen_name\",\"user_description\",\"user_id\"]].groupby(by=[\"user_id\"],axis=0).first()\n",
    "\n",
    "for i, user in voxUsers.iterrows(): \n",
    "    userDict = countEmojis(user[\"user_name\"],userDict)\n",
    "    if not pd.isna(user[\"user_description\"]):\n",
    "        userDict = countEmojis(user[\"user_description\"],userDict)\n",
    "      \n",
    "\n",
    "    \n",
    "duVox = {k: v for k, v in sorted(userDict.items(), reverse=True, key=lambda item: item[1])}\n",
    "duVox = list(duVox.items())\n",
    "finaldu.append(duVox)\n",
    "\n",
    "userDict = {} # Diccionari on guardarem les ocurrencies de cada emoji de tots els usuaris del partit actual.\n",
    "#Pp Users\n",
    "ppUsers = pp[[\"user_name\",\"user_screen_name\",\"user_description\",\"user_id\"]].groupby(by=[\"user_id\"],axis=0).first()\n",
    "\n",
    "for i, user in ppUsers.iterrows(): \n",
    "    userDict = countEmojis(user[\"user_name\"],userDict)\n",
    "    if not pd.isna(user[\"user_description\"]):\n",
    "        userDict = countEmojis(user[\"user_description\"],userDict)\n",
    "duPP = {k: v for k, v in sorted(userDict.items(), reverse=True, key=lambda item: item[1])}\n",
    "duPP = list(duPP.items())\n",
    "finaldu.append(duPP)\n",
    "\n",
    "userDict = {} # Diccionari on guardarem les ocurrencies de cada emoji de tots els usuaris del partit actual.\n",
    "#Ciu Users\n",
    "csUsers = cs[[\"user_name\",\"user_screen_name\",\"user_description\",\"user_id\"]].groupby(by=[\"user_id\"],axis=0).first()\n",
    "\n",
    "for i, user in csUsers.iterrows(): \n",
    "    userDict = countEmojis(user[\"user_name\"],userDict)\n",
    "    if not pd.isna(user[\"user_description\"]):\n",
    "        userDict = countEmojis(user[\"user_description\"],userDict)\n",
    "        \n",
    "duCs = {k: v for k, v in sorted(userDict.items(), reverse=True, key=lambda item: item[1])}\n",
    "duCs = list(duCs.items())\n",
    "finaldu.append(duCs)\n",
    "\n",
    "userDict = {} # Diccionari on guardarem les ocurrencies de cada emoji de tots els usuaris del partit actual.\n",
    "#Psoe Users\n",
    "psoeUsers = psoe[[\"user_name\",\"user_screen_name\",\"user_description\",\"user_id\"]].groupby(by=[\"user_id\"],axis=0).first()\n",
    "\n",
    "for i, user in psoeUsers.iterrows(): \n",
    "    userDict = countEmojis(user[\"user_name\"],userDict)\n",
    "    if not pd.isna(user[\"user_description\"]):\n",
    "        userDict = countEmojis(user[\"user_description\"],userDict)\n",
    "        \n",
    "duPsoe = {k: v for k, v in sorted(userDict.items(), reverse=True, key=lambda item: item[1])}\n",
    "duPsoe = list(duPsoe.items())\n",
    "finaldu.append(duPsoe)\n",
    "\n",
    "userDict = {} \n",
    "#Podemos Users\n",
    "upUsers = up[[\"user_name\",\"user_screen_name\",\"user_description\",\"user_id\"]].groupby(by=[\"user_id\"],axis=0).first()\n",
    "\n",
    "for i, user in upUsers.iterrows(): \n",
    "    userDict = countEmojis(user[\"user_name\"],userDict)\n",
    "    if not pd.isna(user[\"user_description\"]):\n",
    "        userDict = countEmojis(user[\"user_description\"],userDict)\n",
    "        \n",
    "duUP = {k: v for k, v in sorted(userDict.items(), reverse=True, key=lambda item: item[1])}\n",
    "duUP = list(duUP.items())\n",
    "finaldu.append(duUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('üá™üá∏', 64), ('üíö', 23), ('üñ§', 10)], [('üá™üá∏', 23), ('üíô', 16), ('üÖøÔ∏è', 8)], [('üçä', 44), ('üá™üá∏', 43), ('üá™üá∫', 22), ('üñ§', 15), ('üß°', 11), ('üè≥Ô∏è\\u200düåà', 7), ('‚ù§Ô∏è', 4)], [('üåπ', 52), ('‚ù§Ô∏è', 34), ('‚úä', 6), ('üá™üá∏', 6), ('üá™üá∫', 4), ('üîª', 4), ('üíú', 4)], [('üíú', 59), ('üîª', 25), ('‚úä', 16), ('‚ù§Ô∏è', 14), ('üíõ', 14)]]\n"
     ]
    }
   ],
   "source": [
    "endList = []\n",
    "transactions = []\n",
    "for dt in finaldu:\n",
    "    List = []\n",
    "    transaction = []\n",
    "    for i,value in dt:\n",
    "        if value >= 4:\n",
    "            List.append((i,value))\n",
    "            transaction.append(i)\n",
    "    endList.append(List)\n",
    "    transactions.append(transaction)\n",
    "\n",
    "print(endList) #Llista amb els emojis y ocurrencies per a cada partit.\n",
    "\n",
    "\n",
    "flatTrans = []\n",
    "for sublist in transactions:\n",
    "    for item in sublist:\n",
    "        flatTrans.append(item)\n",
    "\n",
    "#Conjunt dels emojis diferents que apareixen (tenint en compte tots els partits politics). Util per quan calculem el tfidf.\n",
    "diffEmojis = set(flatTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proves per a calcular score d'un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'üß°': 1, 'üçä': 1, 'üíô': 1, 'üíõ': 1, 'üîª': 2, 'üá™üá∏': 4, 'üíú': 2, '‚úä': 2, 'üíö': 1, 'üá™üá∫': 2, 'üÖøÔ∏è': 1, 'üè≥Ô∏è\\u200düåà': 1, 'üåπ': 1, 'üñ§': 2, '‚ù§Ô∏è': 3}\n",
      "{0: 0.14722873488772603, 1: 0.10919790808993243, 2: 0.06572036100350014, 3: 0.04549112941256254, 4: 0.5983850308957231}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.15240704, 0.1130386 , 0.06803187, 0.04709114, 0.61943135])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Utilitzarem el tfidf.\n",
    "parties = [\"VOX\", \"PP\", \"CS\", \"PSOE\", \"UP\"]\n",
    "\n",
    "DannyTw = \"Danny üá™üá∏üíúüá™üíõüá∏üá™üá∏\"\n",
    "\n",
    "emojiIdf = {}\n",
    "for emoji in diffEmojis:\n",
    "    emojiIdf[emoji] = 0\n",
    "    for index, party in enumerate(endList):\n",
    "        for partyEemoji, num in party:\n",
    "            if partyEemoji == emoji:\n",
    "                emojiIdf[emoji] += 1\n",
    "                \n",
    "print(emojiIdf)\n",
    "\n",
    "import emoji\n",
    "DannyDict = countEmojis(DannyTw, {})\n",
    "DannyList = list(DannyDict.items())\n",
    "DannyList\n",
    "\n",
    "voxEmojisNum = np.sum([value for i, value in endList[0]])\n",
    "ppEmojisNum = np.sum([value for i, value in endList[1]])\n",
    "csEmojisNum = np.sum([value for i, value in endList[2]])\n",
    "psoeEmojisNum = np.sum([value for i, value in endList[3]])\n",
    "upEmojisNum = np.sum([value for i, value in endList[4]])\n",
    "EmojisPartiesSum = [voxEmojisNum, ppEmojisNum, csEmojisNum, psoeEmojisNum, upEmojisNum]\n",
    "\n",
    "scores = {}\n",
    "for index, party in enumerate(endList):\n",
    "    scores[index] = 0\n",
    "    \n",
    "for i, value in DannyList:\n",
    "    for index, party in enumerate(endList):\n",
    "        for emoji, num in party:\n",
    "            if i == emoji:\n",
    "                scores[index] += value * np.log(len(endList)/emojiIdf[emoji]) * num/EmojisPartiesSum[index]\n",
    "\n",
    "print(scores)\n",
    "list(scores.values())/np.sum(list(scores.values()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracci√≥ keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vox', 1)]\n",
      "{0: 0.3461156800933549, 1: 0, 2: 0, 3: 0, 4: 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxUD = voxUsers[[\"user_name\",\"user_description\"]]\n",
    "ppUD = ppUsers[[\"user_name\",\"user_description\"]]\n",
    "csUD = csUsers[[\"user_name\",\"user_description\"]]\n",
    "psoeUD = psoeUsers[[\"user_name\",\"user_description\"]]\n",
    "upUD = upUsers[[\"user_name\",\"user_description\"]]\n",
    "import emoji\n",
    "def removeStopwords(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('@', '', texto)\n",
    "    texto = re.sub(':', '', texto)\n",
    "    texto = re.sub('/', '', texto)\n",
    "    texto = re.sub('!', '', texto)\n",
    "    texto = re.sub('_', '', texto)\n",
    "    texto = re.sub('\\.', '', texto)\n",
    "    texto = deEmojify(texto)\n",
    "    blob = TextBlob(texto).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('spanish')]\n",
    "    return(outputlist)\n",
    "\n",
    "def normalizeString(l):\n",
    "    l = l.fillna('').values.tolist()\n",
    "    for user in l:\n",
    "        user[0] = removeStopwords(user[0])\n",
    "        user[1] = removeStopwords(user[1]) \n",
    "    return l\n",
    "\n",
    "def wordDict(l):\n",
    "    dt = {}\n",
    "    for sublist in l:\n",
    "        for item in sublist:\n",
    "            for item2 in item:\n",
    "                if item2 not in dt:\n",
    "                    dt[item2] = 1\n",
    "                else:\n",
    "                    dt[item2] += 1\n",
    "    \n",
    "    for key in list(dt.keys()):\n",
    "        if len(key) <= 1:  \n",
    "            del dt[key] \n",
    "    \n",
    "    return dt\n",
    "\n",
    "def countWords(lt):\n",
    "    mainDict = {}\n",
    "    #lt = deEmojify(text).split(' ') # Set deleted\n",
    "    for emoji in lt:\n",
    "        if emoji in mainDict:\n",
    "            mainDict[emoji] += 1\n",
    "        else:\n",
    "            mainDict[emoji] = 1\n",
    "    return mainDict\n",
    "\n",
    "voxWords = normalizeString(voxUD)\n",
    "voxWordsFlat =  wordDict(voxWords)        \n",
    "voxWordsFlat = {k: v for k, v in sorted(voxWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "voxWordsFlat = {k: v for k, v in voxWordsFlat.items() if v > 2}\n",
    "voxWordsFlat = list(voxWordsFlat.items())\n",
    "\n",
    "ppWords = normalizeString(ppUD)\n",
    "ppWordsFlat =  wordDict(ppWords)        \n",
    "ppWordsFlat = {k: v for k, v in sorted(ppWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "ppWordsFlat = {k: v for k, v in ppWordsFlat.items() if v > 2}\n",
    "ppWordsFlat = list(ppWordsFlat.items())\n",
    "\n",
    "csWords = normalizeString(csUD)\n",
    "csWordsFlat =  wordDict(csWords)        \n",
    "csWordsFlat = {k: v for k, v in sorted(csWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "csWordsFlat = {k: v for k, v in csWordsFlat.items() if v > 2}\n",
    "csWordsFlat = list(csWordsFlat.items())\n",
    "\n",
    "psoeWords = normalizeString(psoeUD)\n",
    "psoeWordsFlat =  wordDict(psoeWords)        \n",
    "psoeWordsFlat = {k: v for k, v in sorted(psoeWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "psoeWordsFlat = {k: v for k, v in psoeWordsFlat.items() if v > 2}\n",
    "psoeWordsFlat = list(psoeWordsFlat.items())\n",
    "\n",
    "upWords = normalizeString(upUD)\n",
    "upWordsFlat =  wordDict(upWords)        \n",
    "upWordsFlat = {k: v for k, v in sorted(upWordsFlat.items(), reverse=True, key=lambda item: item[1])}\n",
    "upWordsFlat = {k: v for k, v in upWordsFlat.items() if v > 2}\n",
    "upWordsFlat = list(upWordsFlat.items())\n",
    "#print(ppWordsFlat.keys(Ô∏èÔ∏èÔ∏è)Ô∏èÔ∏èÔ∏è)\n",
    "#for item in list(ppWordsFlat.keys()):\n",
    "    #print(len(item))\n",
    "    \n",
    "    \n",
    "fList = [voxWordsFlat,ppWordsFlat,csWordsFlat,psoeWordsFlat,upWordsFlat]\n",
    "\n",
    "flatTrans = []\n",
    "for sublist in fList:\n",
    "    for item in sublist:\n",
    "        flatTrans.append(item[0])\n",
    "\n",
    "diffWords = set(flatTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proves per a calcular l'score d'un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DannyTw = \"soy de vox\"\n",
    "\n",
    "DannyTwClean = removeStopwords(DannyTw)\n",
    "DannyDict = countWords(DannyTwClean)\n",
    "DannyList = list(DannyDict.items())\n",
    "\n",
    "\n",
    "\n",
    "wordIdf = {}\n",
    "for word in diffWords:\n",
    "    wordIdf[word] = 0\n",
    "    for index, party in enumerate(fList):\n",
    "        for partyWord, num in party:\n",
    "            if partyWord == word:\n",
    "                wordIdf[word] += 1\n",
    "\n",
    "\n",
    "voxWordNum = np.sum([value for i, value in fList[0]])\n",
    "ppWordNum = np.sum([value for i, value in fList[1]])\n",
    "csWordNum = np.sum([value for i, value in fList[2]])\n",
    "psoeWordNum = np.sum([value for i, value in fList[3]])\n",
    "upWordNum = np.sum([value for i, value in fList[4]])\n",
    "WordPartiesSum = [voxWordNum, ppWordNum, csWordNum, psoeWordNum, upWordNum]\n",
    "\n",
    "scores = {}\n",
    "for index, party in enumerate(fList):\n",
    "    scores[index] = 0\n",
    "    \n",
    "print(DannyList)\n",
    "\n",
    "for i, value in DannyList:\n",
    "    for index, party in enumerate(fList):\n",
    "        for word, num in party:\n",
    "            if i == word:\n",
    "                scores[index] += value * np.log(len(fList)/wordIdf[word]) * num/WordPartiesSum[index]\n",
    "\n",
    "print(scores)\n",
    "list(scores.values())/np.sum(list(scores.values()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
